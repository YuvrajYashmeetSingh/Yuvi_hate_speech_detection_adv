{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be7fc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0fe6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...   \n",
      "6  !!!!!!\"@__BrighterDays: I can not just sit up ...   \n",
      "\n",
      "                         labels  \n",
      "0  No Hate and Offensive Speech  \n",
      "1              Offensive Speech  \n",
      "2              Offensive Speech  \n",
      "3              Offensive Speech  \n",
      "4              Offensive Speech  \n",
      "5              Offensive Speech  \n",
      "6              Offensive Speech  \n"
     ]
    }
   ],
   "source": [
    "# Load and preview the data\n",
    "data = pd.read_csv(\"labeled_data.csv\")\n",
    "data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", 1: \"Offensive Speech\", 2: \"No Hate and Offensive Speech\"})\n",
    "data = data[[\"tweet\", \"labels\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3435420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yuvib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize stopwords and stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1170a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean function\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[.?]', '', text)\n",
    "    text = re.sub('https?://\\S+|www.\\S+', '', text)\n",
    "    text = re.sub('<.?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w\\d\\w', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopwords_set]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7326aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)\n",
    "\n",
    "x = np.array(data[\"tweet\"])\n",
    "y = np.array(data[\"labels\"])\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "391b4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2a5f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8974202225210905\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the SVM model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a8580c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: You are too bad and I dont like your attitude\n",
      "SVM Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: It is really awesome\n",
      "SVM Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: fuck you\n",
      "SVM Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: you are an idiot\n",
      "SVM Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: you are killing me\n",
      "SVM Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: go to hell\n",
      "SVM Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: you are looking fucking awesome\n",
      "SVM Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: good morning,\n",
      "SVM Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: fuck you\n",
      "SVM Prediction: ['Offensive Speech']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with example inputs\n",
    "test_inputs = [\n",
    "    \"You are too bad and I dont like your attitude\",\n",
    "    \"It is really awesome\",\n",
    "    \"fuck you\",\n",
    "    \"you are an idiot\",\n",
    "    \"you are killing me\",\n",
    "    \"go to hell\",\n",
    "    \"you are looking fucking awesome\",\n",
    "    \"good morning,\",\"fuck you\"\n",
    "]\n",
    "\n",
    "for inp in test_inputs:\n",
    "    inp_transformed = cv.transform([inp]).toarray()\n",
    "    print(f\"Input: {inp}\")\n",
    "    print(f\"SVM Prediction: {model.predict(inp_transformed)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c3f9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(model, open('svm_hate_speech.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55a3d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CountVectorizer\n",
    "with open('count_vectorizer_svm.pkl', 'wb') as file:\n",
    "    pickle.dump(cv, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10880a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
