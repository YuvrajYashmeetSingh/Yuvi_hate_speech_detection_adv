{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fd992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing with logistic regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e8912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...   \n",
      "6  !!!!!!\"@__BrighterDays: I can not just sit up ...   \n",
      "\n",
      "                         labels  \n",
      "0  No Hate and Offensive Speech  \n",
      "1              Offensive Speech  \n",
      "2              Offensive Speech  \n",
      "3              Offensive Speech  \n",
      "4              Offensive Speech  \n",
      "5              Offensive Speech  \n",
      "6              Offensive Speech  \n"
     ]
    }
   ],
   "source": [
    "# Load and preview the data\n",
    "data = pd.read_csv(\"labeled_data.csv\")\n",
    "data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", 1: \"Offensive Speech\", 2: \"No Hate and Offensive Speech\"})\n",
    "data = data[[\"tweet\", \"labels\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817e4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yuvib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize stopwords and stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a4bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean function\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[.?]', '', text)\n",
    "    text = re.sub('https?://\\S+|www.\\S+', '', text)\n",
    "    text = re.sub('<.?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w\\d\\w', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopwords_set]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ad2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3613cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"tweet\"])\n",
    "y = np.array(data[\"labels\"])\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorizing text data using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c953e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2824ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8954639931531972\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d6c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model with example inputs\n",
    "test_inputs = [\n",
    "    \"You are too bad and I dont like your attitude\",\n",
    "    \"It is really awesome\",\n",
    "    \"fuck you\",\n",
    "    \"you are an idiot\",\n",
    "    \"you are killing me\",\n",
    "    \"go to hell\",\n",
    "    \"you are looking fucking awesome\",\n",
    "    \"good morning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0ec2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: You are too bad and I dont like your attitude\n",
      "Logistic Regression Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: It is really awesome\n",
      "Logistic Regression Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: fuck you\n",
      "Logistic Regression Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: you are an idiot\n",
      "Logistic Regression Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: you are killing me\n",
      "Logistic Regression Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: go to hell\n",
      "Logistic Regression Prediction: ['Offensive Speech']\n",
      "\n",
      "Input: you are looking fucking awesome\n",
      "Logistic Regression Prediction: ['No Hate and Offensive Speech']\n",
      "\n",
      "Input: good morning\n",
      "Logistic Regression Prediction: ['No Hate and Offensive Speech']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp in test_inputs:\n",
    "    inp_transformed = tfidf.transform([inp]).toarray()\n",
    "    print(f\"Input: {inp}\")\n",
    "    print(f\"Logistic Regression Prediction: {model.predict(inp_transformed)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1203a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(model, open('LR_hate_speech.pkl', 'wb'))\n",
    "\n",
    "# Save the CountVectorizer\n",
    "with open('count_vectorizer_LR.pkl', 'wb') as file:\n",
    "    pickle.dump(cv, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
